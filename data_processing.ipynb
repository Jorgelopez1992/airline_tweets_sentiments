{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import re\n",
    "import random\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize,TweetTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer #Haven't used\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data  = pd.read_csv('https://raw.githubusercontent.com/MElHussieni/Twitter-US-Airline-Sentiment-Analysis/master/Tweets.csv')\n",
    "data1 = pd.read_csv('airline_tweets_nov_08-nov_15.csv')\n",
    "data2 = pd.read_csv('nlp_tweets_sample_11_16.csv')\n",
    "data3 = pd.read_csv('airline_tweets_nov_17-nov_24.csv')\n",
    "data4 = pd.read_csv('airline_tweets_nov_24-nov_27.csv')\n",
    "data5 = pd.read_csv(\"airline_tweets_nov_25-Dec_02.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data2.rename(columns={'entities_hashtag':'hashtags',   # renaming columns from api search to match kaggle data\n",
    "                     'created_at': 'tweet_created',\n",
    "                     'coordinates':'tweet_coord',\n",
    "                     'full_text':'text',\n",
    "                     'user_name':'name',\n",
    "                     'id':'tweet_id'})  \n",
    "\n",
    "#sampling 10% of all tweets collected per week to keep dataset size maneagable \n",
    "\n",
    "data1=data1.sample(frac=0.1, replace=True)                 \n",
    "data2=data2.sample(frac=0.1, replace=True)\n",
    "data3=data3.sample(frac=0.1, replace=True)\n",
    "data4=data4.sample(frac=0.1, replace=True)\n",
    "data5=data5.sample(frac=0.1, replace=True)\n",
    "\n",
    "api_tweets = pd.concat([data1,data2,data3,data4,data5])        # merging API dataframes together together \n",
    "\n",
    "del api_tweets['user_screen_name']\n",
    "del api_tweets['Unnamed: 0']\n",
    "\n",
    "tweets=pd.concat([data,api_tweets])                     #merging kaggle and api tweets dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [word_tokenize(tweet) for tweet in tweets[:]['text']]\n",
    "sents = [sent_tokenize(tweet) for tweet in tweets[:]['text']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ Chin-Chia Leong's code ###############\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "tt = TweetTokenizer() #preform better on tweets\n",
    "\n",
    "temp = tweets['text'].apply(tt.tokenize)\n",
    "\n",
    "temp2 = temp.apply(lambda x: [word.lower() for word in x])\n",
    "\n",
    "#need to be lowered to get rid of all the stopwords\n",
    "temp2 = temp2.apply(lambda x: [word for word in x if word not in stop]) \n",
    "temp2 = temp2.apply(lambda x: [word for word in x if word not in stop]) \n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "tweets['full_text_processed'] = temp2.apply(lambda x: [wnl.lemmatize(word) for word in x]) \n",
    "\n",
    "tweets['full_text_processed'] = tweets['full_text_processed'].apply(lambda x: ' ' .join([word for word in x]))\n",
    "\n",
    "###########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_words=[word_tokenize(tweet) for tweet in tweets['full_text_processed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "expletives_list = ['shit','fuck','damn','^hell(?!o)','crap','suck']\n",
    "upper_count=[]\n",
    "caps_ratio=[]\n",
    "num_expletives=[] \n",
    "pun_ratio=[]\n",
    "hashtags=[]\n",
    "\n",
    "regex=\"^#\"                                   # regular expression that matches hashtags\n",
    "\n",
    "for word_list in words:                                 # iterates over every tweet\n",
    "    hashtag='none'\n",
    "    uppers=0                                             # counters to keep track of \n",
    "    expletives=0\n",
    "    pun=0\n",
    "    i=0\n",
    "    for word in word_list:                              # iterates over every word in tweet\n",
    "        \n",
    "        if word.isupper():                               \n",
    "            uppers+=1                                     # adds 1 to count if uppercase word\n",
    "        \n",
    "        if word.isalnum()==0:                             # adds files               \n",
    "            pun+=1\n",
    "            \n",
    "        for expletive in expletives_list:                    # uses regex to find number of times people swear\n",
    "            \n",
    "            if bool(re.search(expletive,word.lower())):\n",
    "                expletives+=1\n",
    "            \n",
    "                \n",
    "        if bool(re.search(regex,word.lower())):           #word tokenize broke apart hashtags so we must look \n",
    "            try:\n",
    "                hash_word=word_list[i+1].lower()                      # one word ahead to find hashtag\n",
    "                if hashtag!=0:\n",
    "                    hashtag='#'+hash_word+' '+hashtag \n",
    "                else:\n",
    "                    hashtag='#'+hash_word\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "        i+=1\n",
    "        \n",
    "    ratio = uppers/len(word_list)                             # ratio of uppercase/(num_word)\n",
    "    pun = pun/len(word_list)                                  # ratio of punctuation/(num_word)\n",
    "    hashtags.append(hashtag)\n",
    "    upper_count.append(uppers)\n",
    "    num_expletives.append(expletives)\n",
    "    caps_ratio.append(ratio)\n",
    "    pun_ratio.append(pun)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['sents'] = sents                    # sentences in each word\n",
    "tweets['words'] = words                    # words in each tweets, NOT PROCESSED \n",
    "tweets['upper_count'] = upper_count        # number of words that are uppercase\n",
    "tweets['caps_ratio'] = caps_ratio          # ratio of words that are uppercase\n",
    "tweets['num_expletives'] = num_expletives  # number of expletives\n",
    "tweets['pun_ratio'] = pun_ratio            # ratio of words that are punctuation \n",
    "tweets['clean_words'] = processed_words     # words in lowercase and after lemmanization \n",
    "tweets['hashtags'] = hashtags                # hashtags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating bag of words matrix for each tweet using processed text\n",
    "matrix = CountVectorizer(max_features=2500)\n",
    "X = matrix.fit_transform(tweets['full_text_processed']).toarray()\n",
    "\n",
    "#making dataframe for bag of words and merging \n",
    "X = pd.DataFrame(X)\n",
    "X.columns = matrix.get_feature_names() # \n",
    "\n",
    "X.reset_index(drop=True,inplace=True)\n",
    "tweets.reset_index(drop=True,inplace=True)\n",
    "\n",
    "tweets = pd.concat([tweets,X],axis=1)\n",
    "\n",
    "#creating bag of words for hashtags\n",
    "matrix = CountVectorizer(max_features=2500)\n",
    "Y = matrix.fit_transform(tweets['hashtags']).toarray()\n",
    "hash_list = matrix.get_feature_names()\n",
    "hash_list = ['#'+hashtag for hashtag in hash_list]\n",
    "Y = pd.DataFrame(Y)\n",
    "Y.columns = hash_list\n",
    "Y.reset_index(drop=True,inplace=True)\n",
    "\n",
    "tweets = pd.concat([tweets,Y],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data = tweets[0:14640]\n",
    "api_data = tweets[14641:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data.to_csv(\"labeled_data.csv\",sep=\",\",encoding='utf-8')\n",
    "api_data.to_csv(\"api_data.csv\",sep=\",\",encoding='utf-8')\n",
    "tweets.to_csv(\"tweets_processed.csv\",sep=\",\",encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>...</th>\n",
       "      <th>#yousuck</th>\n",
       "      <th>#youtube</th>\n",
       "      <th>#youweredoingsowell</th>\n",
       "      <th>#yow</th>\n",
       "      <th>#yuck</th>\n",
       "      <th>#yucki</th>\n",
       "      <th>#yxe</th>\n",
       "      <th>#yyc</th>\n",
       "      <th>#zrh</th>\n",
       "      <th>#zurich</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14635</th>\n",
       "      <td>569587686496825344</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3487</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KristenReenders</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14636</th>\n",
       "      <td>569587371693355008</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Customer Service Issue</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>itsropes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14637</th>\n",
       "      <td>569587242672398336</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sanyabun</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14638</th>\n",
       "      <td>569587188687634433</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Customer Service Issue</td>\n",
       "      <td>0.6659</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SraJackson</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14639</th>\n",
       "      <td>569587140490866689</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6771</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>daviddtwu</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14640 rows × 5028 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0      570306133677760513           neutral                        1.0000   \n",
       "1      570301130888122368          positive                        0.3486   \n",
       "2      570301083672813571           neutral                        0.6837   \n",
       "3      570301031407624196          negative                        1.0000   \n",
       "4      570300817074462722          negative                        1.0000   \n",
       "...                   ...               ...                           ...   \n",
       "14635  569587686496825344          positive                        0.3487   \n",
       "14636  569587371693355008          negative                        1.0000   \n",
       "14637  569587242672398336           neutral                        1.0000   \n",
       "14638  569587188687634433          negative                        1.0000   \n",
       "14639  569587140490866689           neutral                        0.6771   \n",
       "\n",
       "               negativereason  negativereason_confidence         airline  \\\n",
       "0                         NaN                        NaN  Virgin America   \n",
       "1                         NaN                     0.0000  Virgin America   \n",
       "2                         NaN                        NaN  Virgin America   \n",
       "3                  Bad Flight                     0.7033  Virgin America   \n",
       "4                  Can't Tell                     1.0000  Virgin America   \n",
       "...                       ...                        ...             ...   \n",
       "14635                     NaN                     0.0000        American   \n",
       "14636  Customer Service Issue                     1.0000        American   \n",
       "14637                     NaN                        NaN        American   \n",
       "14638  Customer Service Issue                     0.6659        American   \n",
       "14639                     NaN                     0.0000        American   \n",
       "\n",
       "      airline_sentiment_gold             name negativereason_gold  \\\n",
       "0                        NaN          cairdin                 NaN   \n",
       "1                        NaN         jnardino                 NaN   \n",
       "2                        NaN       yvonnalynn                 NaN   \n",
       "3                        NaN         jnardino                 NaN   \n",
       "4                        NaN         jnardino                 NaN   \n",
       "...                      ...              ...                 ...   \n",
       "14635                    NaN  KristenReenders                 NaN   \n",
       "14636                    NaN         itsropes                 NaN   \n",
       "14637                    NaN         sanyabun                 NaN   \n",
       "14638                    NaN       SraJackson                 NaN   \n",
       "14639                    NaN        daviddtwu                 NaN   \n",
       "\n",
       "       retweet_count  ... #yousuck #youtube #youweredoingsowell #yow #yuck  \\\n",
       "0                  0  ...        0        0                   0    0     0   \n",
       "1                  0  ...        0        0                   0    0     0   \n",
       "2                  0  ...        0        0                   0    0     0   \n",
       "3                  0  ...        0        0                   0    0     0   \n",
       "4                  0  ...        0        0                   0    0     0   \n",
       "...              ...  ...      ...      ...                 ...  ...   ...   \n",
       "14635              0  ...        0        0                   0    0     0   \n",
       "14636              0  ...        0        0                   0    0     0   \n",
       "14637              0  ...        0        0                   0    0     0   \n",
       "14638              0  ...        0        0                   0    0     0   \n",
       "14639              0  ...        0        0                   0    0     0   \n",
       "\n",
       "      #yucki #yxe #yyc #zrh #zurich  \n",
       "0          0    0    0    0       0  \n",
       "1          0    0    0    0       0  \n",
       "2          0    0    0    0       0  \n",
       "3          0    0    0    0       0  \n",
       "4          0    0    0    0       0  \n",
       "...      ...  ...  ...  ...     ...  \n",
       "14635      0    0    0    0       0  \n",
       "14636      0    0    0    0       0  \n",
       "14637      0    0    0    0       0  \n",
       "14638      0    0    0    0       0  \n",
       "14639      0    0    0    0       0  \n",
       "\n",
       "[14640 rows x 5028 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
